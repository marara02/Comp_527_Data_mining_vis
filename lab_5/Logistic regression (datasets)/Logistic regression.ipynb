{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "electronic-scheme",
   "metadata": {},
   "source": [
    "# Logistic Regression (Spam Email Classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-tulsa",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this lab we will develop a Spam email classifier using Logistic Regression.\n",
    "\n",
    "We will use [SPAM E-mail Database](https://www.kaggle.com/somesh24/spambase) from Kaggle, which was split into two almost equal parts: training dataset (train.csv) and test dataset (test.csv).\n",
    "Each record in the datasets contains 58 features, one of which is the class label. The class label is the last feature and it takes two values +1 (spam email) and -1 (non-spam email). The other features represent various characteristics of emails such as frequencies of certain words or characters in the text of an email; and lengths of sequences of consecutive capital letters (See [SPAM E-mail Database](https://www.kaggle.com/somesh24/spambase) for the detailed description of the features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "latter-queens",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-03-12T09:29:53.109112Z",
     "start_time": "2024-03-12T09:29:51.360621Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distant-monte",
   "metadata": {},
   "source": [
    "We start with implementing some auxiliary functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "subjective-router",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T09:29:56.037343Z",
     "start_time": "2024-03-12T09:29:56.033198Z"
    }
   },
   "outputs": [],
   "source": [
    "# Implement sigmoid function\n",
    "def sigmoid(x):\n",
    "    # Bound the argument to be in the interval [-500, 500] to prevent overflow\n",
    "    x = np.clip( x, -500, 500 )\n",
    "\n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "unnecessary-limit",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T09:29:58.223170Z",
     "start_time": "2024-03-12T09:29:58.217078Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(fname):\n",
    "    labels = []\n",
    "    features = []\n",
    "    \n",
    "    with open(fname) as F:\n",
    "        next(F) # skip the first line with feature names\n",
    "        for line in F:\n",
    "            p = line.strip().split(',')\n",
    "            labels.append(int(p[-1]))\n",
    "            features.append(np.array(p[:-1], float))\n",
    "    return (np.array(labels), np.array(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenging-analyst",
   "metadata": {},
   "source": [
    "Next we read the training and the test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "interested-somewhere",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T09:30:02.723991Z",
     "start_time": "2024-03-12T09:30:02.685965Z"
    }
   },
   "outputs": [],
   "source": [
    "(trainingLabels, trainingData) = load_data(\"train.csv\")\n",
    "(testLabels, testData) = load_data(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "racial-halloween",
   "metadata": {},
   "source": [
    "In the files the positive objects appear before the negative objects. So we reshuffle both datasets to avoid situation when we present to our training algorithm all positive objects and then all negative objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "376e3eae-5829-4d2c-93d1-c7d26eec65b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T09:30:07.891496Z",
     "start_time": "2024-03-12T09:30:07.885864Z"
    }
   },
   "outputs": [],
   "source": [
    "#Reshuffle training data and\n",
    "permutation =  np.random.permutation(len(trainingData))\n",
    "trainingLabels = trainingLabels[permutation] # Y_train\n",
    "trainingData = trainingData[permutation] # X_train\n",
    "\n",
    "#test data\n",
    "permutation =  np.random.permutation(len(testData))\n",
    "testLabels = testLabels[permutation]\n",
    "testData = testData[permutation]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-pioneer",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "1. Implement Logistic Regression training algorithm."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "((2301, 57), (2301,))"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingData.shape, trainingLabels.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T09:30:13.113976Z",
     "start_time": "2024-03-12T09:30:13.107029Z"
    }
   },
   "id": "6947408f5ef9e7cf",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "entire-jason",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T09:38:20.027595Z",
     "start_time": "2024-03-12T09:38:20.016380Z"
    }
   },
   "outputs": [],
   "source": [
    "def logisticRegression(trainingData, trainingLabels, learningRate, maxIter):\n",
    "    #Compute the number of training objects\n",
    "    numTrainingObj = len(trainingData)\n",
    "    #Compute the number of features (dimension of our data)\n",
    "    numFeatures = len(trainingData[0])\n",
    "    \n",
    "    #Initialize the bias term and the weights\n",
    "    b = 0\n",
    "    W = np.zeros(numFeatures)\n",
    "    \n",
    "    for t in range(maxIter):\n",
    "        #For every training object\n",
    "        for i in range(numTrainingObj):\n",
    "            X = trainingData[i]\n",
    "            y = trainingLabels[i]\n",
    "            #Compute the activation score\n",
    "            a = np.dot(X, W) + b\n",
    "        \n",
    "            #Update the bias term and the weights\n",
    "            b = b + learningRate*y*sigmoid(-y*a)\n",
    "            for s in range(numFeatures):\n",
    "                W[s] = W[s] + learningRate*y*sigmoid(-y*a)*X[s]\n",
    "            \n",
    "            #The above for-loop can be equivalently written in the vector form as follows\n",
    "            #W = np.add(W, learningRate*y*sigmoid(-y*a)*X)\n",
    "            \n",
    "    return (b, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-spring",
   "metadata": {},
   "source": [
    "2. Use the training dataset to train Logistic Regression classifier. Use learningRate=0.1 and maxIter=10. Output the bias term and the weight vector of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "smaller-medicaid",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T09:38:26.238077Z",
     "start_time": "2024-03-12T09:38:21.442337Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias term:  -359.241556756663 \n",
      "Weight vector:  [  -3.06886154 -152.73004915  -12.64407314   49.487       -10.60851097\n",
      "   27.87071877   56.86345449   42.60101464   14.74856774  -14.0462899\n",
      "    9.94805629 -202.24803715   12.42251917  -14.60491616   21.995\n",
      "   96.49507949   30.90826492   18.78978616  -42.40899094   24.81674505\n",
      "   25.36285248   93.09200247   65.12942993   43.9920461  -570.28970002\n",
      " -264.27336196 -252.23389513 -139.35554652 -109.55133003  -96.222\n",
      "  -58.802       -36.34604031 -115.44951341  -37.64904031 -119.26876728\n",
      "  -72.62911283 -140.99608899   -1.76103242  -89.36814206  -33.59799997\n",
      "  -55.92059139 -148.25537505  -54.69810707  -66.51215011 -144.87530257\n",
      "  -72.66167569   -8.101       -42.22314051   -7.13607185  -72.58142022\n",
      "  -10.88241216  106.38525455   38.89858458   26.87524034 -180.46758984\n",
      "   95.72074018    6.52413413]\n"
     ]
    }
   ],
   "source": [
    "(b,W) = logisticRegression(trainingData, trainingLabels, 0.1, 10)\n",
    "print(\"Bias term: \", b, \"\\nWeight vector: \", W) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-yesterday",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "1. Implement Logistic Regression classifier with given bias term and weight vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "sufficient-brief",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T09:38:59.620134Z",
     "start_time": "2024-03-12T09:38:59.612432Z"
    }
   },
   "outputs": [],
   "source": [
    "def logisticRegressionTest(b, W, X):\n",
    "    #Compute the activation score\n",
    "    a = np.dot(X, W) + b\n",
    "    predictedClass = 0;\n",
    "    confidence = 0;\n",
    "    \n",
    "    if a > 0:\n",
    "        predictedClass = +1\n",
    "        confidence = sigmoid(a)\n",
    "    else:\n",
    "        predictedClass = -1\n",
    "        confidence = 1-sigmoid(a)\n",
    "    return (predictedClass, confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprised-broadcast",
   "metadata": {},
   "source": [
    "2. Use the trained model to classify objects in the test dataset. Output an evaluation report (accuracy, precision, recall, F-score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "analyzed-example",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T09:39:40.851510Z",
     "start_time": "2024-03-12T09:39:40.847790Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluationReport(classTrue, classPred):\n",
    "    positive_mask = classTrue == 1\n",
    "\n",
    "    # Count the number of elements in the positive class \n",
    "    positive = np.count_nonzero(positive_mask)\n",
    "    # Count True Positive\n",
    "    tp = np.count_nonzero(classPred[positive_mask]==1)\n",
    "    # Count False Negative\n",
    "    fn = np.count_nonzero(classPred[positive_mask]==-1)\n",
    "    \n",
    "    negative_mask = classTrue == -1\n",
    "\n",
    "    # Count the number of elements in the negative class \n",
    "    negative = np.count_nonzero(negative_mask)\n",
    "    # Count False Positive\n",
    "    fp = np.count_nonzero(classPred[negative_mask]==1)\n",
    "    # Count True Negative\n",
    "    tn = np.count_nonzero(classPred[negative_mask]==-1)\n",
    "\n",
    "    # Compute Accuracy, Precision, Recall, and F-score\n",
    "    accuracy = (tp + tn)/(tp + tn + fp + fn)\n",
    "    precision = tp/(tp + fp)\n",
    "    recall = tp/(tp + fn)\n",
    "    fscore = 2*precision*recall/(precision + recall)\n",
    "    print(\"Evaluation report\")\n",
    "    print(\"Accuracy: %.2f\" % accuracy)\n",
    "    print(\"Precision: %.2f\" % precision)\n",
    "    print(\"Recall: %.2f\" % recall)\n",
    "    print(\"F-score: %.2f\" % fscore)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation report\n",
      "Accuracy: 0.72\n",
      "Precision: 0.58\n",
      "Recall: 0.96\n",
      "F-score: 0.72\n"
     ]
    }
   ],
   "source": [
    "classTrue = np.array([int(x) for x in testLabels], dtype=int)\n",
    "classPred = np.array([int(logisticRegressionTest(b,W,X)[0]) for X in testData], dtype=int)\n",
    "evaluationReport(classTrue, classPred)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T09:40:02.879125Z",
     "start_time": "2024-03-12T09:40:02.846527Z"
    }
   },
   "id": "87f3933e2724aa0",
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "exciting-citizen",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "1. Apply Gaussian Normalisation to the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "disabled-ordering",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T09:40:21.634629Z",
     "start_time": "2024-03-12T09:40:21.628454Z"
    }
   },
   "outputs": [],
   "source": [
    "def GaussianNormalisation(dataset):\n",
    "    #Compute the number of features\n",
    "    numFeatures = len(dataset[0])\n",
    "    \n",
    "    featureMean = np.empty(numFeatures, float)\n",
    "    featureStd = np.empty(numFeatures, float)\n",
    "    \n",
    "    #For every feature\n",
    "    for i in range(numFeatures):\n",
    "        #find its Mean and Std\n",
    "        featureMean[i] = dataset[:,i].mean(axis=0)\n",
    "        featureStd[i] = dataset[:,i].std(axis=0)\n",
    "        #Apply Gaussian Noramlisation\n",
    "        dataset[:,i] = (dataset[:,i] - featureMean[i])/featureStd[i]\n",
    "\n",
    "    return (featureMean, featureStd)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#normalize the training dataset\n",
    "(featureMean, featureStd) = GaussianNormalisation(trainingData)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T09:40:37.438049Z",
     "start_time": "2024-03-12T09:40:37.430845Z"
    }
   },
   "id": "854446f5a82a77c1",
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "id": "brutal-christian",
   "metadata": {},
   "source": [
    "2. Train Logistic Regression on the normalised training dataset. Use learningRate=0.1 and maxIter=10. Output the bias term and the weight vector of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "inside-shirt",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T09:41:04.932138Z",
     "start_time": "2024-03-12T09:41:00.325004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias term:  -3.014954248709226 \n",
      "Weight vector:  [-3.33368536e-01 -4.01986832e-01 -3.09385814e-02  1.07389500e+00\n",
      "  7.12869732e-01  3.09112536e-01  3.92537393e-01  5.63239982e-01\n",
      "  1.97221668e-01  5.75237149e-01 -1.04425635e-01 -5.27376751e-01\n",
      " -4.23379043e-01  1.45851777e-01  3.35020087e-02  1.41728519e+00\n",
      "  8.36202497e-01  1.74503081e-01  5.18954936e-01 -3.25973707e-02\n",
      " -3.60703547e-03  2.18416437e+00  8.07646042e-01  1.18910995e+00\n",
      " -4.22275485e+00 -1.25417651e+00 -5.78280911e+00  2.34425724e-01\n",
      " -1.09135154e+00 -5.20967628e-01 -1.22232600e+00 -1.37664584e-02\n",
      " -4.48857893e-01 -6.31292325e-01 -1.40147458e+00  4.46475941e-01\n",
      " -7.01842697e-01  2.43574011e-01 -8.34645247e-02 -1.67400843e-01\n",
      " -3.19399377e+00 -1.85706739e+00 -2.94014522e-01 -9.50337895e-01\n",
      " -9.40782712e-01 -4.51683904e-01 -4.54988036e-01 -1.27990303e+00\n",
      "  2.22783245e-01  1.58320828e-01 -2.23687215e-01  1.42230315e+00\n",
      "  1.75162905e+00  1.27062282e+00  4.56505206e-01  1.08100633e+00\n",
      "  1.10981980e+00]\n"
     ]
    }
   ],
   "source": [
    "#Train Logistic Regression classifier on the normalised training data\n",
    "(b,W) = logisticRegression(trainingData, trainingLabels, 0.1, 10)\n",
    "print(\"Bias term: \", b, \"\\nWeight vector: \", W) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "north-parts",
   "metadata": {},
   "source": [
    "3. Normalise the test dataset using Means and Standard Deviations of the features *computed on the training dataset*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "seven-garage",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T09:41:35.613719Z",
     "start_time": "2024-03-12T09:41:35.598362Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalise(dataset, featureMean, featureStd):\n",
    "    #Compute the number of features\n",
    "    numFeatures = len(dataset[0])\n",
    "    \n",
    "    #For every feature\n",
    "    for i in range(numFeatures):\n",
    "        #Apply Gaussian Noramlisation with given Mean and Std values\n",
    "        dataset[:,i] = (dataset[:,i] - featureMean[i])/featureStd[i]"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#normalize the test dataset using Means and Std computed on the training dataset\n",
    "normalise(testData, featureMean, featureStd)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T09:41:53.117802Z",
     "start_time": "2024-03-12T09:41:53.111269Z"
    }
   },
   "id": "adc6dd6012c35f09",
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "id": "failing-narrow",
   "metadata": {},
   "source": [
    "4. Use the model trained on the normalised training dataset to classify objects in the normalised test dataset. Output an evaluation report (accuracy, precision, recall, F-score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "electronic-invention",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T09:42:07.084787Z",
     "start_time": "2024-03-12T09:42:07.064695Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation report\n",
      "Accuracy: 0.89\n",
      "Precision: 0.81\n",
      "Recall: 0.94\n",
      "F-score: 0.87\n"
     ]
    }
   ],
   "source": [
    "#Predict class labels of test objects for the normalized test dataset\n",
    "classPred = np.array([int(logisticRegressionTest(b,W,X)[0]) for X in testData], dtype=int)\n",
    "evaluationReport(classTrue, classPred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-canal",
   "metadata": {},
   "source": [
    "5. Compare the quality of the classifier with normalisation and without normalisation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
