{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "electronic-scheme",
   "metadata": {},
   "source": [
    "# Logistic Regression (Spam Email Classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-tulsa",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this lab we will develop a Spam email classifier using Logistic Regression.\n",
    "\n",
    "We will use [SPAM E-mail Database](https://www.kaggle.com/somesh24/spambase) from Kaggle, which was split into two almost equal parts: training dataset (train.csv) and test dataset (test.csv).\n",
    "Each record in the datasets contains 58 features, one of which is the class label. The class label is the last feature and it takes two values +1 (spam email) and -1 (non-spam email). The other features represent various characteristics of emails such as frequencies of certain words or characters in the text of an email; and lengths of sequences of consecutive capital letters (See [SPAM E-mail Database](https://www.kaggle.com/somesh24/spambase) for the detailed description of the features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "latter-queens",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distant-monte",
   "metadata": {},
   "source": [
    "We start with implementing some auxiliary functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "subjective-router",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement sigmoid function\n",
    "def sigmoid(x):\n",
    "    # Bound the argument to be in the interval [-500, 500] to prevent overflow\n",
    "    x = np.clip( x, -500, 500 )\n",
    "\n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "unnecessary-limit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(fname):\n",
    "    labels = []\n",
    "    features = []\n",
    "    \n",
    "    with open(fname) as F:\n",
    "        next(F) # skip the first line with feature names\n",
    "        for line in F:\n",
    "            p = line.strip().split(',')\n",
    "            labels.append(int(p[-1]))\n",
    "            features.append(np.array(p[:-1], float))\n",
    "    return (np.array(labels), np.array(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenging-analyst",
   "metadata": {},
   "source": [
    "Next we read the training and the test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "interested-somewhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingLabels, trainingData) = load_data(\"train.csv\")\n",
    "(testLabels, testData) = load_data(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "racial-halloween",
   "metadata": {},
   "source": [
    "In the files the positive objects appear before the negative objects. So we reshuffle both datasets to avoid situation when we present to our training algorithm all positive objects and then all negative objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "infinite-chapel",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshuffle training data and\n",
    "permutation =  np.random.permutation(len(trainingData))\n",
    "trainingLabels = trainingLabels[permutation]\n",
    "trainingData = trainingData[permutation]\n",
    "\n",
    "#test data\n",
    "permutation =  np.random.permutation(len(testData))\n",
    "testLabels = testLabels[permutation]\n",
    "testData = testData[permutation]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-pioneer",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "1. Implement Logistic Regression training algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-jason",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-spring",
   "metadata": {},
   "source": [
    "2. Use the training dataset to train Logistic Regression classifier. Use learningRate=0.1 and maxIter=10. Output the bias term and the weight vector of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-medicaid",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-yesterday",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "1. Implement Logistic Regression classifier with given bias term and weight vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sufficient-brief",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "surprised-broadcast",
   "metadata": {},
   "source": [
    "2. Use the trained model to classify objects in the test dataset. Output an evaluation report (accuracy, precision, recall, F-score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyzed-example",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "exciting-citizen",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "1. Apply Gaussian Normalisation to the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-ordering",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "brutal-christian",
   "metadata": {},
   "source": [
    "2. Train Logistic Regression on the normalised training dataset. Use learningRate=0.1 and maxIter=10. Output the bias term and the weight vector of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-shirt",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "north-parts",
   "metadata": {},
   "source": [
    "3. Normalise the test dataset using Means and Standard Deviations of the features *computed on the training dataset*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-garage",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "failing-narrow",
   "metadata": {},
   "source": [
    "4. Use the model trained on the normalised training dataset to classify objects in the normalised test dataset. Output an evaluation report (accuracy, precision, recall, F-score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-invention",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "right-canal",
   "metadata": {},
   "source": [
    "5. Compare the quality of the classifier with normalisation and without normalisation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
